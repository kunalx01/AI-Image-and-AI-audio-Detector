import librosa
import numpy as np
from transformers import Wav2Vec2Processor, Wav2Vec2Model
import torch
from test import classify_audio

# Load Wav2Vec2 model and processor
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h")


def preprocess_audio(file_path, target_sr=16000, duration=5.0):
    """Pre-process audio by resampling, truncating/padding, and converting to mono."""
    # Load audio (preserve original sample rate)
    y, sr = librosa.load(file_path, sr=None, mono=False)

    # Convert to mono if needed
    if y.ndim > 1:
        y = librosa.to_mono(y)

    # Resample to target sample rate
    if sr != target_sr:
        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)

    # Truncate or pad audio to fixed duration
    max_len = int(target_sr * duration)
    if len(y) > max_len:
        y = y[:max_len]
    else:
        y = np.pad(y, (0, max_len - len(y)))

    # Normalize audio between -1 and 1
    y = librosa.util.normalize(y)

    # Handle NaN values if any
    y = np.nan_to_num(y)

    return y, target_sr


def extract_features_for_llm(audio_file):
    classify_result = classify_audio(audio_file)

    # Pre-process the audio to ensure correct format and sample rate
    y, sr = preprocess_audio(audio_file, target_sr=16000, duration=5.0)

    # 1. MFCCs (Mel-Frequency Cepstral Coefficients) - Take mean and std
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    mfcc_mean = np.mean(mfccs, axis=1)
    mfcc_std = np.std(mfccs, axis=1)

    # 2. Spectral Centroid - Take mean and std
    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
    centroid_mean = np.mean(spectral_centroid)
    centroid_std = np.std(spectral_centroid)

    # 3. Spectral Bandwidth - Take mean and std
    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)
    bandwidth_mean = np.mean(spectral_bandwidth)
    bandwidth_std = np.std(spectral_bandwidth)

    # 4. Zero Crossing Rate (ZCR) - Take mean
    zcr = librosa.feature.zero_crossing_rate(y)
    zcr_mean = np.mean(zcr)

    # 5. RMS (Root Mean Square Energy) - Take mean and std
    rms = librosa.feature.rms(y=y)
    rms_mean = np.mean(rms)
    rms_std = np.std(rms)

    # 6. Wav2Vec Embeddings - Use mean of last hidden state (shape: [batch_size, time_steps, embedding_size])
    inputs = processor(y, sampling_rate=sr, return_tensors="pt", padding=True)
    with torch.no_grad():
        embeddings = model(**inputs).last_hidden_state
    wav2vec_mean = torch.mean(embeddings, dim=1).squeeze().numpy()

    # Prepare summary for LLM prompt, keeping each feature separate
    summary = {
        "MFCCs Mean": mfcc_mean.tolist(),
        "MFCCs Std": mfcc_std.tolist(),
        "Spectral Centroid Mean": centroid_mean,
        "Spectral Centroid Std": centroid_std,
        "Spectral Bandwidth Mean": bandwidth_mean,
        "Spectral Bandwidth Std": bandwidth_std,
        "Zero Crossing Rate Mean": zcr_mean,
        "RMS Mean": rms_mean,
        "RMS Std": rms_std,
        "Wav2Vec Mean Embeddings": wav2vec_mean[:10].tolist(),  # Truncated for compactness
        "AI Detection Result": classify_result  # Include classify_audio result in the summary
    }

    return summary


def format_prompt_for_llm(summary):
    """Format extracted features into a compact prompt for the LLM."""
    prompt = f"""
    Audio Analysis Report:
    - MFCCs Mean: {summary['MFCCs Mean']}
    - MFCCs Std: {summary['MFCCs Std']}
    - Spectral Centroid Mean: {summary['Spectral Centroid Mean']:.2f}
    - Spectral Centroid Std: {summary['Spectral Centroid Std']:.2f}
    - Spectral Bandwidth Mean: {summary['Spectral Bandwidth Mean']:.2f}
    - Spectral Bandwidth Std: {summary['Spectral Bandwidth Std']:.2f}
    - Zero Crossing Rate Mean: {summary['Zero Crossing Rate Mean']:.4f}
    - RMS Mean: {summary['RMS Mean']:.4f}
    - RMS Std: {summary['RMS Std']:.4f}
    - Wav2Vec Mean Embeddings (Truncated): {summary['Wav2Vec Mean Embeddings']}
    
    AI Detection Result:
    - AI Generated: {summary['AI Detection Result']['is_ai_generated']}
    - Confidence: {summary['AI Detection Result']['confidence']:.2f}
    - CNN Result: {summary['AI Detection Result']['cnn_result']}
    - TCN Result: {summary['AI Detection Result']['tcn_result']}
    
    Based on the above analysis, is the audio generated by AI or not? Just say True or False.
    """
    return prompt


if __name__ == "__main__":
    audio_file = "data\\test_audio.wav"  # Replace with your audio file
    features_summary = extract_features_for_llm(audio_file)
    prompt = format_prompt_for_llm(features_summary)
    
    print(prompt)  # Use this prompt in your LLM query
